# Connect to DashDB
library(ibmdbR)
mycon <- idaConnect("BLUDB", "", "")
idaInit(mycon)

# get table from database
DEBATE <- as.data.frame(ida.data.frame('"DASH5022"."V_DEBATE"')[ ,c('CANDIDATE', 'CANDIDATE_CONFIDENCE', 'CANDIDATE_GOLD', 'ID', 'NAME', 'RELEVANT_YN', 'RELEVANT_YN_CONFIDENCE', 'RELEVANT_YN_GOLD', 'RETWEET_COUNT', 'SENTIMENT', 'SENTIMENT_CONFIDENCE', 'SENTIMENT_GOLD', 'SUBJECT_MATTER', 'SUBJECT_MATTER_CONFIDENCE', 'SUBJECT_MATTER_GOLD', 'TEXT', 'TWEET_COORD', 'TWEET_CREATED', 'TWEET_ID', 'TWEET_LOCATION', 'USER_TIMEZONE')])

# make sure table was loaded correctly
summary(DEBATE)
dim(DEBATE)
str(DEBATE)

#*****************************DATA PREPROCESSING********************************************************

# convert to appropriate data type
attach(DEBATE)
CANDIDATE<-as.factor(CANDIDATE)
CANDIDATE_CONFIDENCE<-as.integer(CANDIDATE_CONFIDENCE)
RELEVANT_YN<-as.factor(RELEVANT_YN)
RELEVANT_YN_CONFIDENCE<-as.integer(RELEVANT_YN_CONFIDENCE)
SENTIMENT<-as.factor(SENTIMENT)
SENTIMENT_CONFIDENCE<-as.integer(SENTIMENT_CONFIDENCE)
RELEVANT_YN<-as.factor(RELEVANT_YN)
SUBJECT_MATTER<-as.factor(SUBJECT_MATTER)
SUBJECT_MATTER_CONFIDENCE<-as.integer(SUBJECT_MATTER_CONFIDENCE)

# get only the text
some_txt = DEBATE$TEXT

# look at text before cleaning
some_txt[15]

# remove unnecessary spaces
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)

# remove retweet
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)

# remove at people
some_txt = gsub("@\\w+", "", some_txt)

# remove punctuation
some_txt = gsub("[[:punct:]]", "", some_txt)

# remove numbers
some_txt = gsub("[[:digit:]]", "", some_txt)

# remove html links
some_txt = gsub("http\\w+", "", some_txt)

# replace words that have the same meaning and need to be counted as one
some_txt = gsub("debates", "debate", some_txt)
some_txt = gsub("stands", "stand", some_txt)
some_txt = gsub("primaries", "primary", some_txt)
some_txt = gsub("votes", "vote", some_txt)
some_txt = gsub("tedcruz", "cruz", some_txt)
some_txt = gsub("donaldtrump", "trump", some_txt)
some_txt = gsub("hilaryclinton", "clinton", some_txt)

# remove unnecessary spaces
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)

# define "tolower error handling" function 
try.error = function(x)
{
  # create missing value
  y = NA
  # tryCatch error
  try_error = tryCatch(tolower(x), error=function(e) e)
  # if not an error
  if (!inherits(try_error, "error"))
    y = tolower(x)
  # result
  return(y)
}
# lower case using try.error with sapply 
some_txt = sapply(some_txt, try.error)

# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL

# look at text after cleaning
some_txt[15]

#Replace full stop and comma
sentences<-gsub("\\.","",some_txt)
sentences<-gsub("\\,","",some_txt)

# transform into a data frame
# try without this: sentences.df<-as.data.frame(sentences)

# Split sentences into words
words<-strsplit(sentences," ")
words <- unlist(words)

# get word frequencies using the table function
words.freq<-table(words)

#convert to data frame
words.df<-as.data.frame(words.freq)

# look at the data frame to verify it is correct
head(words.df)

# look at most frequent words
head(words.df[order(words.df$Freq, decreasing=TRUE), ], 50) # most of the top words are common words so they need to be removed for this to be valuable

# remove stopwords 
# gopdebate removed because doesn't add value 
words.rem<-words.df[ ! words.df$words %in% c("gopdebate","gopdebates","did", "ask","said","im","just","i", "amp","a", "about", "above", "above", "across", "after", "afterwards", "again", "against", "all", "almost", "alone", "along", "already", "also","although","always","am","among", "amongst", "amoungst", "amount",  "an", "and", "another", "any","anyhow","anyone","anything","anyway", "anywhere", "are", "around", "as",  "at", "back","be","became", "because","become","becomes", "becoming", "been", "before", "beforehand", "behind", "being", "below", "beside", "besides", "between", "beyond", "bill", "both", "bottom","but", "by", "call", "can", "cannot", "cant", "co", "con", "could", "couldnt", "cry", "de", "describe", "detail", "do", "done", "down", "due", "during", "each", "eg", "eight", "either", "eleven","else", "elsewhere", "empty", "enough", "etc", "even", "ever", "every", "everyone", "everything", "everywhere", "except", "few", "fifteen", "fify", "fill", "find", "fire", "first", "five", "for", "former", "formerly", "forty", "found", "four", "from", "front", "full", "further", "get", "give", "go", "had", "has", "hasnt", "have", "he", "hence", "her", "here", "hereafter", "hereby", "herein", "hereupon", "hers", "herself", "him", "himself", "his", "how", "however", "hundred", "ie", "if", "in", "inc", "indeed", "interest", "into", "is", "it", "its", "itself", "keep", "last", "latter", "latterly", "least", "less", "ltd", "made", "many", "may", "me", "meanwhile", "might", "mill", "mine", "more", "moreover", "most", "mostly", "move", "much", "must", "my", "myself", "name", "namely", "neither", "never", "nevertheless", "next", "nine", "no", "nobody", "none", "noone", "nor", "not", "nothing", "now", "nowhere", "of", "off", "often", "on", "once", "one", "only", "onto", "or", "other", "others", "otherwise", "our", "ours", "ourselves", "out", "over", "own","part", "per", "perhaps", "please", "put", "rather", "re", "same", "see", "seem", "seemed", "seeming", "seems", "serious", "several", "she", "should", "show", "side", "since", "sincere", "six", "sixty", "so", "some", "somehow", "someone", "something", "sometime", "sometimes", "somewhere", "still", "such", "system", "take", "ten", "than", "that", "the", "their", "them", "themselves", "then", "thence", "there", "thereafter", "thereby", "therefore", "therein", "thereupon", "these", "they", "thickv", "thin", "third", "this", "those", "though", "three", "through", "throughout", "thru", "thus", "to", "together", "too", "top", "toward", "towards", "twelve", "twenty", "two", "un", "under", "until", "up", "upon", "us", "very", "via", "was", "we", "well", "were", "what", "whatever", "when", "whence", "whenever", "where", "whereafter", "whereas", "whereby", "wherein", "whereupon", "wherever", "whether", "which", "while", "whither", "who", "whoever", "whole", "whom", "whose", "why", "will", "with", "within", "without", "would", "yet", "you", "your", "yours", "yourself", "yourselves", "the"), ]

#***********************************WORD FREQUENCY*****************************************

# view most frequent words after stop words were removed
head(words.rem[order(words.rem$Freq, decreasing=TRUE), ], 70) # much better

# create new variable sorted for nicer plot
words.rem.sorted<-words.rem[order(words.rem$Freq, decreasing=TRUE), ]

# frequency plot of words with frequency more than 25
p<-ggplot(subset(words.rem.sorted, Freq>30), aes(reorder(words, -Freq),Freq))
p<-p+ geom_bar(stat="identity", colour="black", fill="royalblue")
p<-p+ ggtitle("Frequency of Words")
p<-p+ theme(axis.text.x = element_text(angle=45, hjust = 1))
p<-p+ labs(x="Words", y="Frequency")
all_freq_plot<-p
all_freq_plot

# get a list of the words in the plot becausde plot is a bit fuzzy
words.rem.sorted[words.rem.sorted$Freq>30,]

#********************************ASSOCIATION RULES*****************************************

# find association rules with apriori
rules <- apriori(sentences.df)
